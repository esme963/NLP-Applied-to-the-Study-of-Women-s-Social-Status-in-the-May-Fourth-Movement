# -*- coding: utf-8 -*-
"""Debert-v3-large.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wsAbtDmec35St3lxNuIN_dueIyzqRrf9
"""

# sample:encomyï¼Œand others is same, only the keywords and the topic_text is different

import os
import torch
import numpy as np
import matplotlib.pyplot as plt
from transformers import DebertaV2Tokenizer, DebertaV2Model
from sklearn.metrics.pairwise import cosine_similarity

# Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize DeBERTa model and tokenizer
tokenizer = DebertaV2Tokenizer.from_pretrained('microsoft/deberta-v3-base')
model = DebertaV2Model.from_pretrained('microsoft/deberta-v3-base').to(device)

# Keyword set
keywords = ["economy", "career", "profession", "industry", "factory", "workshop", "industrial", "business", "merchant", "company"]

# Compute DeBERTa embedding vectors
def get_deberta_embedding(text):
    inputs = tokenizer(text, return_tensors='pt').to(device)
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()

# Calculate similarity between text and keyword set
def calculate_similarity(text_embedding, keywords):
    keywords_embeddings = [get_deberta_embedding(keyword) for keyword in keywords]
    keywords_mean_embedding = np.mean(keywords_embeddings, axis=0)
    return cosine_similarity(text_embedding, keywords_mean_embedding.reshape(1, -1))[0][0]

# Process each text and calculate sentiment value
def analyze_texts(texts):
    sentiment_scores = []
    for idx, text in enumerate(texts):
        text = ''.join(text)
        text_embedding = get_deberta_embedding(text)
        sentiment_value = calculate_similarity(text_embedding, keywords)
        sentiment_scores.append(sentiment_value)
        if idx % 100 == 0:
            print(f"Processed {idx} texts.")
    return np.mean(sentiment_scores) if sentiment_scores else 0

# Process data for each year and calculate average sentiment value
def process_yearly_data(base_path, years):
    average_sentiments = []
    for year in years:
        file_path = os.path.join(base_path, f"{year}.txt")
        with open(file_path, 'r', encoding='utf-8') as file:
            texts = file.read().splitlines()
        avg_sentiment = analyze_texts(texts)
        average_sentiments.append(avg_sentiment)
        print(f"Processed year {year}. Average sentiment: {avg_sentiment}")
    return average_sentiments

# Set path and range of years
base_path = '/kaggle/input/categorized-output-new/Economy_and_Social_Status'
years = list(range(1915, 1932))

# Process data and calculate average sentiment value for each year
average_sentiments = process_yearly_data(base_path, years)

# Plot the sentiment value changes
plt.figure(figsize=(12, 6))
plt.plot(years, average_sentiments, marker='o', linestyle='-', color='b')
plt.title('Average Sentiment Value for Economy (1915-1931)')
plt.xlabel('Year')
plt.ylabel('Average Sentiment Value')
plt.grid(True)
plt.xticks(years, rotation=45)
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

# Create DataFrame
df = pd.DataFrame({
    'Year': years,
    'Average_Sentiment': average_sentiments
})

# Save to CSV file
df.to_csv('Economy.csv', index=False)

# Normalize the data
scaler = MinMaxScaler()
average_sentiments_normalized = scaler.fit_transform(np.array(average_sentiments).reshape(-1, 1)).flatten()

# Plot the normalized line chart
plt.figure(figsize=(12, 6))
plt.plot(years, average_sentiments_normalized, marker='o', linestyle='-', color='b')
plt.title('Normalized Average Sentiment Value for Economy (1915-1931)')
plt.xlabel('Year')
plt.ylabel('Normalized Average Sentiment Value')
plt.grid(True)
plt.xticks(years, rotation=45)
plt.tight_layout()
plt.savefig('Economy (1915-1931).png')
plt.show()

