# -*- coding: utf-8 -*-
"""Hyphenate Tokenization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c1Xiza_V8mUMJytVo8_EqbsrUj30SQ-3
"""

#Jieba
import pandas as pd
import jieba.posseg as pseg
from gensim import corpora, models
import re

# Read txt file
with open('/content/processed_dataset.txt', 'r', encoding='utf-8') as file:
    text = file.read()
words_pos = pseg.cut(text)

with open('Jieba_SegmentedText.txt', 'w', encoding='utf-8') as output_file:
    for word, pos in words_pos:
        output_file.write(f"{word}|") # Outputs only words, separated by tabs

#Topwords-seg
#R language
# R package requires R 3.4 for Windows and MacOS and R 3.2.3 for Linux.
# You can use anaconda to manage versions of R.
# The R package for TopWORDS can currently be installed using the following steps.

# 1. Install anaconda 2. Install R3.4
conda create -n R3.4
conda activate R3.4
conda install -c r r=3.4


# 3. Install the TopWORDS package
install.packages("Rcpp")
#windows
install.packages("http://tsing.v-dk.com/kdeng/wp-content/uploads/sites/2/2017/11/TopWORDS_1.1.zip",repos=NUll,type="win.binary")
#mac
install.packages("http://tsing.v-dk.com/kdeng/wp-content/uploads/sites/2/2017/11/TopWORDS_1.1_binart_mac.zip",repos=NUll,type="mac.binary")
#linux
install.packages("http://tsing.v-dk.com/kdeng/wp-content/uploads/sites/2/2017/11/TopWORDS_1.1_binart_linux.zip",repos=NUll)

# 4. Run the R script
library(TopWORDS)
getwd()
topwords("/content/processed_dataset.txt",MaxWordLen = 8, MinWordFreq = 5)

#After running successfully, the topwords function outputs 5 text files as a result:
# TopWORDS_log.txt is the program's log file, recording the details of the program running;
# TopWORDS_ltemSet.txt lists the number of occurrences of a word;
# TopWORDS_CompleteDict.txt lists the complete dictionary generated from program initialization;
# TopWORDS_WordDict.txt lists the program's final dictionary through optimization and model selection;
# Topwords_SegmentedText.txt gives the final text segmentation result.

import jieba
import jieba.posseg as pseg
from collections import defaultdict

jieba.initialize()  # Initialize Jieba tokenizer
hmm_tagger = pseg.POSTokenizer()  # Initialize Jieba POS tagger (HMM-based)

# greedy matching
def greedy_match(jieba_tokens, topwords_tokens):
    matched_tokens = []
    i, j = 0, 0
    while i < len(jieba_tokens) and j < len(topwords_tokens):
        jieba_token = jieba_tokens[i]
        topwords_token = topwords_tokens[j]

        # Greedily select the longer token
        if len(jieba_token) >= len(topwords_token):
            matched_tokens.append(jieba_token)
            i += 1
        else:
            matched_tokens.append(topwords_token)
            j += 1

    # Append remaining tokens if any
    if i < len(jieba_tokens):
        matched_tokens.extend(jieba_tokens[i:])
    elif j < len(topwords_tokens):
        matched_tokens.extend(topwords_tokens[j:])

    return matched_tokens

# POS tagging function
def pos_tagging(matched_tokens):
    pos_tagged = []

    for token in matched_tokens:
        # Check if token is in Jieba dictionary
        words = pseg.cut(token, use_paddle=False)  # Jieba POS tagging
        for word, flag in words:
            pos_tagged.append((word, flag))

    return pos_tagged

# process files
def process_text_files(jieba_file, topwords_file, output_file):
    # Load tokenized text from Jieba and Topwords files
    with open(jieba_file, 'r', encoding='utf-8') as f_jieba, open(topwords_file, 'r', encoding='utf-8') as f_topwords:
        jieba_tokens = f_jieba.read().split()  # Tokenized Jieba text
        topwords_tokens = f_topwords.read().split()  # Tokenized Topwords text

    # Perform greedy matching
    matched_tokens = greedy_match(jieba_tokens, topwords_tokens)

    # Perform POS tagging
    pos_tagged = pos_tagging(matched_tokens)

    # Save the output to a file
    with open(output_file, 'w', encoding='utf-8') as f_output:
        for word, pos in pos_tagged:
            f_output.write(f"{word}/{pos} ")

jieba_segmented_text = "Jieba_SegmentedText.txt"
topwords_segmented_text = "Topwords_SegmentedText.txt"
output_pos_tagged_text = "Tokenized_POSTagged_Text.txt"

process_text_files(jieba_segmented_text, topwords_segmented_text, output_pos_tagged_text)