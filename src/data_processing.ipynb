{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_file_path = '/content/text_sorted_confidence.zip'\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/personal_project')  # Unzip to the specified directory\n"
      ],
      "metadata": {
        "id": "cyyi4bn47FyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "# Define the range of years\n",
        "years = range(1915, 1932)\n",
        "\n",
        "# Initialize data structures\n",
        "yearly_counts = defaultdict(int)\n",
        "yearly_counts_09 = defaultdict(int)\n",
        "word_counts = defaultdict(int)\n",
        "word_counts_09 = defaultdict(int)\n",
        "min_word_counts = defaultdict(lambda: float('inf'))\n",
        "max_word_counts = defaultdict(lambda: float('-inf'))\n",
        "min_word_counts_09 = defaultdict(lambda: float('inf'))\n",
        "max_word_counts_09 = defaultdict(lambda: float('-inf'))\n",
        "\n",
        "# Root directory path\n",
        "root_path = '/content/personal_project/text_sorted_confidence'\n",
        "\n",
        "# Iterate through all subfolders\n",
        "for folder in os.listdir(root_path):\n",
        "    folder_path = os.path.join(root_path, folder)\n",
        "\n",
        "    if os.path.isdir(folder_path):\n",
        "        # Iterate through all txt files in the folder\n",
        "        for file_name in os.listdir(folder_path):\n",
        "            if file_name.endswith('.txt'):\n",
        "                # Extract the year\n",
        "                year_prefix = file_name[:2]\n",
        "                year = 1900 + int(year_prefix)\n",
        "\n",
        "                if year in years:\n",
        "                    file_path = os.path.join(folder_path, file_name)\n",
        "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                        content = file.read()\n",
        "                        word_count = len(content)  # Assume the text content is already tokenized\n",
        "\n",
        "                    # Increase the number of files and word count for the corresponding year\n",
        "                    yearly_counts[year] += 1\n",
        "                    word_counts[year] += word_count\n",
        "\n",
        "                    # Update the minimum and maximum word count\n",
        "                    if word_count < min_word_counts[year]:\n",
        "                        min_word_counts[year] = word_count\n",
        "                    if word_count > max_word_counts[year]:\n",
        "                        max_word_counts[year] = word_count\n",
        "\n",
        "                    # If inside the 0.9 folder, increase the 0.9 file count and word count for the corresponding year\n",
        "                    if folder == '0.9':\n",
        "                        yearly_counts_09[year] += 1\n",
        "                        word_counts_09[year] += word_count\n",
        "\n",
        "                        # Update the minimum and maximum word count for the 0.9 folder\n",
        "                        if word_count < min_word_counts_09[year]:\n",
        "                            min_word_counts_09[year] = word_count\n",
        "                        if word_count > max_word_counts_09[year]:\n",
        "                            max_word_counts_09[year] = word_count\n",
        "\n",
        "# Output the results\n",
        "for year in years:\n",
        "    total_count = yearly_counts[year]\n",
        "    count_09 = yearly_counts_09[year]\n",
        "    total_word_count = word_counts[year]\n",
        "    word_count_09 = word_counts_09[year]\n",
        "    min_word_count = min_word_counts[year] if min_word_counts[year] != float('inf') else 0\n",
        "    max_word_count = max_word_counts[year] if max_word_counts[year] != float('-inf') else 0\n",
        "    min_word_count_09 = min_word_counts_09[year] if min_word_counts_09[year] != float('inf') else 0\n",
        "    max_word_count_09 = max_word_counts_09[year] if max_word_counts_09[year] != float('-inf') else 0\n",
        "    proportion_09 = count_09 / total_count if total_count > 0 else 0\n",
        "    word_proportion_09 = word_count_09 / total_word_count if total_word_count > 0 else 0\n",
        "\n",
        "    print(f\"Year: {year}, Total Count: {total_count}, 0.9 Count: {count_09}, 0.9 Proportion: {proportion_09:.2%}\")\n",
        "    print(f\"Total Word Count: {total_word_count}, 0.9 Word Count: {word_count_09}, 0.9 Word Proportion: {word_proportion_09:.2%}\")\n",
        "    print(f\"Min Word Count: {min_word_count}, Max Word Count: {max_word_count}\")\n",
        "    print(f\"Min Word Count 0.9: {min_word_count_09}, Max Word Count 0.9: {max_word_count_09}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "5J7d8s-f7Fvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "# Define the directory path\n",
        "main_directory_09 = '/content/personal_project/data'\n",
        "sub_dirs = [main_directory_09]\n",
        "\n",
        "# Used to store the merged data\n",
        "merged_data = defaultdict(list)\n",
        "\n",
        "# Iterate through subdirectories\n",
        "for dir_path in sub_dirs:\n",
        "    for file_name in os.listdir(dir_path):\n",
        "        if file_name.endswith('.txt'):\n",
        "            year_page, page_part = file_name.split('_')\n",
        "            year = '19' + year_page[:2]\n",
        "            page = int(year_page[2:])\n",
        "            part = int(page_part.split('.')[0])\n",
        "\n",
        "            file_path = os.path.join(dir_path, file_name)\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "\n",
        "            # Store file content\n",
        "            merged_data[year].append((page, part, file_name, content))\n",
        "\n",
        "# Sort by year, page number, and part number, and merge content\n",
        "output_directory = '/content/personal_project/merged_files'\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "for year, files in sorted(merged_data.items()):\n",
        "    year_dir = os.path.join(output_directory, year)\n",
        "    os.makedirs(year_dir, exist_ok=True)\n",
        "\n",
        "    sorted_files = sorted(files, key=lambda x: (x[0], x[1]))\n",
        "\n",
        "    for _, _, file_name, content in sorted_files:\n",
        "        output_file_path = os.path.join(year_dir, file_name)\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "            output_file.write(content)\n",
        "\n",
        "print('File merging and sorting completed!')\n"
      ],
      "metadata": {
        "id": "wq2xTnfd7Fr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Specify the folder path\n",
        "folder_path = '/content/personal_project/merged_files/1915'\n",
        "\n",
        "# Initialize a string to store file content\n",
        "all_content = \"\"\n",
        "\n",
        "# Get and sort all .txt files in the folder\n",
        "files = sorted([f for f in os.listdir(folder_path) if f.endswith('.txt')])\n",
        "\n",
        "# Iterate through all .txt files in the folder\n",
        "for filename in files:\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "        all_content += content + \" \"  # Merge into the total content\n",
        "\n",
        "# Split the merged content into chunks of 300 characters\n",
        "def split_into_chunks(text, chunk_size=300):\n",
        "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "# Split the merged content\n",
        "segment_1915 = split_into_chunks(all_content, chunk_size=300)\n"
      ],
      "metadata": {
        "id": "zsARaRjr7FdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(segment_1915))"
      ],
      "metadata": {
        "id": "S1Q6l8-q6VZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data filtering\n",
        "from transformers import DebertaModel, DebertaTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the pre-trained DeBERTa model and tokenizer\n",
        "model_name = 'microsoft/deberta-base'\n",
        "tokenizer = DebertaTokenizer.from_pretrained(model_name)\n",
        "model = DebertaModel.from_pretrained(model_name)\n",
        "\n",
        "# Define the word to compare with\n",
        "female_word = '女性'  # \"female\" in Chinese\n",
        "\n",
        "# Function to get the text vector\n",
        "def get_segment_vector(segment, tokenizer, model):\n",
        "    inputs = tokenizer(segment, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "    outputs = model(**inputs)\n",
        "    # Get the average of all token vectors as the text vector\n",
        "    segment_vector = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "    return segment_vector\n",
        "\n",
        "# Get the vector for the word \"female\"\n",
        "female_vector = get_segment_vector(female_word, tokenizer, model)\n",
        "\n",
        "# Calculate the similarity between each text and \"female\" and filter out texts with similarity less than 0.8\n",
        "filtered_texts = []\n",
        "similarities = []\n",
        "for idx, segment in enumerate(segment_1915):\n",
        "    segment_vector = get_segment_vector(segment.strip(), tokenizer, model)\n",
        "    similarity = cosine_similarity(female_vector, segment_vector)[0][0]\n",
        "    similarities.append(similarity)\n",
        "    print(f\"Text: {segment.strip()}\\nSimilarity with 'female': {similarity}\\n\")\n",
        "\n",
        "    if similarity >= 0.8:\n",
        "        filtered_texts.append(segment.strip())\n",
        "\n",
        "    # Print a progress message every 500 texts processed\n",
        "    if idx % 500 == 0:\n",
        "        print(f\"Processed {idx + 1} texts so far\")\n",
        "\n",
        "# Output the final dataset size\n",
        "final_size = len(filtered_texts)\n",
        "print(f\"Final dataset size: {final_size}\")\n",
        "\n",
        "# Name the filtered dataset\n",
        "segment_1915 = filtered_texts\n"
      ],
      "metadata": {
        "id": "69RJBde36W_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Data cleaning function\n",
        "def preprocess_data(text):\n",
        "    # Use regular expressions to remove special characters and numbers\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'\\s+', '', text)  # Remove whitespace\n",
        "    # Return the preprocessed text\n",
        "    return text\n",
        "\n",
        "# Clean the text data in segment_1915\n",
        "segment_1915_clean = []\n",
        "for segment in segment_1915:\n",
        "    data = preprocess_data(segment)\n",
        "    segment_1915_clean.append(data)\n",
        "\n",
        "# Save the cleaned data to a text file\n",
        "with open('clean_segment_1915.txt', 'w', encoding='utf-8') as file:\n",
        "    for data in segment_1915_clean:\n",
        "        file.write(data + '\\n')\n",
        "\n",
        "print(\"Cleaned data saved successfully!\")"
      ],
      "metadata": {
        "id": "4S0KU-gH6YmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(segment_1915_clean[0])"
      ],
      "metadata": {
        "id": "Z3HgGhsi6d9V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}